{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c27ade69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"all-minilm\"\n",
    ") # By default uses the llama2 model:b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58e3792d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=embeddings.embed_documents([\n",
    "    \"Hello world\",\n",
    "    \"Bye world\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9ce1c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40feb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0265ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# embeddings = HuggingFaceEmbeddings(\n",
    "#     model_name=\"all-MiniLM-L6-v2\"\n",
    "# )\n",
    "\n",
    "# result = embeddings.embed_documents([\"Hello world\", \"Bye world\"])\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc0c99db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='OpenAI’s GPT-4 Turbo model offers a cost-effective alternative to GPT-4 with higher performance and a significantly extended context length of up to 128,000 tokens. While it supports most of the same capabilities as GPT-4, Turbo has usage limits depending on the API plan. For example, the free-tier ChatGPT users can send up to 40 messages every 3 hours, while Plus users are allotted up to 100 messages per 3 hours. These limits are enforced to balance performance across users and may change depending on demand and system stability. Additionally, developers integrating Turbo via the API may encounter rate limits such as tokens-per-minute (TPM) and requests-per-minute (RPM), which can be adjusted by applying for rate limit increases.' metadata={'source': 'sample.txt'}\n"
     ]
    }
   ],
   "source": [
    "# VectorStore FAISS = Facebook AI Similarity Search\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "\n",
    "loader=TextLoader(\"sample.txt\")\n",
    "documents=loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs=text_splitter.split_documents(documents)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"all-minilm\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49ac7115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI’s GPT-4 Turbo model offers a cost-effective alternative to GPT-4 with higher performance and a significantly extended context length of up to 128,000 tokens. While it supports most of the same capabilities as GPT-4, Turbo has usage limits depending on the API plan. For example, the free-tier ChatGPT users can send up to 40 messages every 3 hours, while Plus users are allotted up to 100 messages per 3 hours. These limits are enforced to balance performance across users and may change depending on demand and system stability. Additionally, developers integrating Turbo via the API may encounter rate limits such as tokens-per-minute (TPM) and requests-per-minute (RPM), which can be adjusted by applying for rate limit increases.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does GPT-4 Turbo compare to GPT-4 in terms of training data, update frequency, and architectural differences?\"\n",
    "docs = vectorstore.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97f18cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(id='b6f85280-cf15-4713-8531-fcea561aa7e4', metadata={'source': 'sample.txt'}, page_content='OpenAI’s GPT-4 Turbo model offers a cost-effective alternative to GPT-4 with higher performance and a significantly extended context length of up to 128,000 tokens. While it supports most of the same capabilities as GPT-4, Turbo has usage limits depending on the API plan. For example, the free-tier ChatGPT users can send up to 40 messages every 3 hours, while Plus users are allotted up to 100 messages per 3 hours. These limits are enforced to balance performance across users and may change depending on demand and system stability. Additionally, developers integrating Turbo via the API may encounter rate limits such as tokens-per-minute (TPM) and requests-per-minute (RPM), which can be adjusted by applying for rate limit increases.'), np.float32(12.031931))]\n"
     ]
    }
   ],
   "source": [
    "# Retriver QA\n",
    "\n",
    "retriver= vectorstore.as_retriever()\n",
    "docs=retriver.invoke(query)\n",
    "docs[0].page_content\n",
    "\n",
    "\n",
    "docs_and_scores = vectorstore.similarity_search_with_score(query)\n",
    "print(docs_and_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e26702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI’s GPT-4 Turbo model offers a cost-effective alternative to GPT-4 with higher performance and a significantly extended context length of up to 128,000 tokens. While it supports most of the same capabilities as GPT-4, Turbo has usage limits depending on the API plan. For example, the free-tier ChatGPT users can send up to 40 messages every 3 hours, while Plus users are allotted up to 100 messages per 3 hours. These limits are enforced to balance performance across users and may change depending on demand and system stability. Additionally, developers integrating Turbo via the API may encounter rate limits such as tokens-per-minute (TPM) and requests-per-minute (RPM), which can be adjusted by applying for rate limit increases.\n"
     ]
    }
   ],
   "source": [
    "## Using Chromadb as a vectorstore\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"all-minilm\"\n",
    ")\n",
    "\n",
    "docsearch = Chroma.from_documents(documents, embeddings)    \n",
    "\n",
    "query = \"?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
